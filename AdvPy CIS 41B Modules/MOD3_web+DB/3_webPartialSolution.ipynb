{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 3: web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import urllib.request as ur\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Example of using urllib to access web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n",
      "<class 'bytes'>\n",
      "\n",
      "Header content:\n",
      "text/html; charset=ISO-8859-1\n",
      "\n",
      "Dictionary of header:\n",
      "Date: Tue, 16 May 2023 19:28:28 GMT\n",
      "Expires: -1\n",
      "Cache-Control: private, max-age=0\n",
      "Content-Type: text/html; charset=ISO-8859-1\n",
      "Content-Security-Policy-Report-Only: object-src 'none';base-uri 'self';script-src 'nonce-RqhNwJXl74AFNSxUr_8RlQ' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp\n",
      "P3P: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\n",
      "Server: gws\n",
      "X-XSS-Protection: 0\n",
      "X-Frame-Options: SAMEORIGIN\n",
      "Set-Cookie: 1P_JAR=2023-05-16-19; expires=Thu, 15-Jun-2023 19:28:28 GMT; path=/; domain=.google.com; Secure\n",
      "Set-Cookie: AEC=AUEFqZfQx9UOJHuANWB5Ecqg5Z68d8kNKoTK9Mm3BnLksV0GH7SFFgvKjA; expires=Sun, 12-Nov-2023 19:28:28 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=lax\n",
      "Set-Cookie: NID=511=rJtFfFeIzO61d4NDbw8ntJ1nqGyb4LBYA3emewKEF36a9nTQnSunBgOmrSzGGFKpGetjdCZNY4JIoZ_BqIDhZgQ217KCHkC0YZnqFfg7XB7UkYL13Kdv_UzZkfPT-qe_EAqiUkAsCY_krEx4dPOiT-4tIK_1-wKliUvWljDJq3w; expires=Wed, 15-Nov-2023 19:28:28 GMT; path=/; domain=.google.com; HttpOnly\n",
      "Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\n",
      "Accept-Ranges: none\n",
      "Vary: Accept-Encoding\n",
      "Connection: close\n",
      "Transfer-Encoding: chunked\n",
      "\n",
      "Another way to get header info:\n",
      "Date: Tue, 16 May 2023 19:28:28 GMT\n",
      "Expires: -1\n",
      "Cache-Control: private, max-age=0\n",
      "Content-Type: text/html; charset=ISO-8859-1\n",
      "Content-Security-Policy-Report-Only: object-src 'none';base-uri 'self';script-src 'nonce-RqhNwJXl74AFNSxUr_8RlQ' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp\n",
      "P3P: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\n",
      "Server: gws\n",
      "X-XSS-Protection: 0\n",
      "X-Frame-Options: SAMEORIGIN\n",
      "Set-Cookie: 1P_JAR=2023-05-16-19; expires=Thu, 15-Jun-2023 19:28:28 GMT; path=/; domain=.google.com; Secure\n",
      "Set-Cookie: AEC=AUEFqZfQx9UOJHuANWB5Ecqg5Z68d8kNKoTK9Mm3BnLksV0GH7SFFgvKjA; expires=Sun, 12-Nov-2023 19:28:28 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=lax\n",
      "Set-Cookie: NID=511=rJtFfFeIzO61d4NDbw8ntJ1nqGyb4LBYA3emewKEF36a9nTQnSunBgOmrSzGGFKpGetjdCZNY4JIoZ_BqIDhZgQ217KCHkC0YZnqFfg7XB7UkYL13Kdv_UzZkfPT-qe_EAqiUkAsCY_krEx4dPOiT-4tIK_1-wKliUvWljDJq3w; expires=Wed, 15-Nov-2023 19:28:28 GMT; path=/; domain=.google.com; HttpOnly\n",
      "Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\n",
      "Accept-Ranges: none\n",
      "Vary: Accept-Encoding\n",
      "Connection: close\n",
      "Transfer-Encoding: chunked\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = ur.urlopen(\"https://www.google.com/\")\n",
    "print(type(page))  # url Response object\n",
    "print(type(page.read()))  # read entire page as string of bytes or ascii characters\n",
    "print(\"\\nHeader content:\")\n",
    "print(page.getheader(\"Content-Type\"))  # type of data:  text/html; charset=ISO8859-1\n",
    "print(\"\\nDictionary of header:\")\n",
    "for key, value in page.getheaders():  # see all headers\n",
    "    print(key + \":\", value)\n",
    "print(\"\\nAnother way to get header info:\")\n",
    "print(page.info())  # see all headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Example of using requests to access web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<class 'requests.models.Response'>\n",
      "<class 'str'>\n",
      "<class 'bytes'>\n",
      "text/html; charset=utf-8\n",
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"https://python.org/\")\n",
    "print(page.status_code)  # status code = 200 means success\n",
    "print(type(page))\n",
    "text = page.text  # get text from the Response object\n",
    "print(type(text))\n",
    "content = page.content\n",
    "print(type(content))\n",
    "print(page.headers[\"Content-Type\"])  # type of data\n",
    "print(page.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use beautifulsoup to get data from web page. Add to the code below to print the first 1000 characters of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>   <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\">   <![endif]-->\n",
      "<!--[if IE 7]>      <html class=\"no-js ie7 lt-ie8 lt-ie9\">          <![endif]-->\n",
      "<!--[if IE 8]>      <html class=\"no-js ie8 lt-ie9\">                 <![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <!-- Google tag (gtag.js) -->\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-TF35YF9CVH\">\n",
      "  </script>\n",
      "  <script>\n",
      "   window.dataLayer = window.dataLayer || [];\n",
      "      function gtag(){dataLayer.push(arguments);}\n",
      "      gtag('js', new Date());\n",
      "      gtag('config', 'G-TF35YF9CVH');\n",
      "  </script>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <link href=\"//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js\" rel=\"prefetch\"/>\n",
      "  <link href=\"//ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js\" rel=\"prefetch\"/>\n",
      "  <meta content=\"Python.org\" name=\"application-name\"/>\n",
      "  <meta con\n"
     ]
    }
   ],
   "source": [
    "# Use beautifulsoup to get data from web page. \n",
    "# Add to the code below to print the first 1000 characters of the page.\n",
    "page = requests.get(\"https://python.org/\")\n",
    "soup = BeautifulSoup(page.content, \"lxml\")\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find all tag names with 'b' in the name (use regex) and print just the tag name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>Web Development</b>\n",
      "b\n",
      "Web Development\n",
      "<b>GUI Development</b>\n",
      "b\n",
      "GUI Development\n",
      "<b>Scientific and Numeric</b>\n",
      "b\n",
      "Scientific and Numeric\n",
      "<b>Software Development</b>\n",
      "b\n",
      "Software Development\n",
      "<b>System Administration</b>\n",
      "b\n",
      "System Administration\n",
      "\n",
      "body\n",
      "label\n",
      "button\n",
      "blockquote\n",
      "table\n",
      "tbody\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"b\"):  # tag is exactly 'b'\n",
    "    print(tag)\n",
    "    print(tag.name)\n",
    "    print(tag.text)\n",
    "\n",
    "print()\n",
    "\n",
    "for tag in soup.find_all(re.compile(\"b\")):  # tag contains 'b' somewhere\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find all tag names that are 'image' or 'table' and print the entire tag element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"quote-from\" width=\"100%\">\n",
      "<tbody>\n",
      "<tr>\n",
      "<td><p><a href=\"/success-stories/saving-the-world-with-open-data-and-python/\">Saving the world with Open Data and Python</a> <em>by James Baster</em></p></td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all([\"image\", \"table\"]):  # 'image' is not a valid tag so nothing shows up\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find and print all https links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.python.org/psf/\n",
      "https://docs.python.org\n",
      "https://pypi.org/\n",
      "https://psfmember.org/civicrm/contribute/transact?reset=1&id=2\n",
      "https://www.facebook.com/pythonlang?fref=ts\n",
      "https://twitter.com/ThePSF\n",
      "http://brochure.getpython.info/\n",
      "https://docs.python.org/3/license.html\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://devguide.python.org/\n",
      "https://docs.python.org/faq/\n",
      "http://wiki.python.org/moin/Languages\n",
      "http://python.org/dev/peps/\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://wiki.python.org/moin/\n",
      "http://pyfound.blogspot.com/\n",
      "http://pycon.blogspot.com/\n",
      "http://planetpython.org/\n",
      "https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\n",
      "http://docs.python.org/3/tutorial/introduction.html#using-python-as-a-calculator\n",
      "https://docs.python.org\n",
      "https://blog.python.org\n",
      "https://pyfound.blogspot.com/2023/05/psf-board-election-dates-for-2023.html\n",
      "https://pyfound.blogspot.com/2023/05/python-humble-bundle.html\n",
      "https://pyfound.blogspot.com/2023/04/thank-you-for-many-years-of-service-van.html\n",
      "https://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\n",
      "https://pythoninsider.blogspot.com/2023/04/its-time-for-another-set-of-python.html\n",
      "http://www.djangoproject.com/\n",
      "http://www.pylonsproject.org/\n",
      "http://bottlepy.org\n",
      "http://tornadoweb.org\n",
      "http://flask.pocoo.org/\n",
      "http://www.web2py.com/\n",
      "http://wiki.python.org/moin/TkInter\n",
      "https://wiki.gnome.org/Projects/PyGObject\n",
      "http://www.riverbankcomputing.co.uk/software/pyqt/intro\n",
      "https://wiki.qt.io/PySide\n",
      "https://kivy.org/\n",
      "http://www.wxpython.org/\n",
      "http://www.scipy.org\n",
      "http://pandas.pydata.org/\n",
      "http://ipython.org\n",
      "http://buildbot.net/\n",
      "http://trac.edgewall.org/\n",
      "http://roundup.sourceforge.net/\n",
      "http://www.ansible.com\n",
      "https://saltproject.io\n",
      "https://www.openstack.org\n",
      "https://xon.sh\n",
      "http://brochure.getpython.info/\n",
      "https://docs.python.org/3/license.html\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://devguide.python.org/\n",
      "https://docs.python.org/faq/\n",
      "http://wiki.python.org/moin/Languages\n",
      "http://python.org/dev/peps/\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://wiki.python.org/moin/\n",
      "http://pyfound.blogspot.com/\n",
      "http://pycon.blogspot.com/\n",
      "http://planetpython.org/\n",
      "https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\n",
      "https://devguide.python.org/\n",
      "https://bugs.python.org/\n",
      "https://mail.python.org/mailman/listinfo/python-dev\n",
      "https://github.com/python/pythondotorg/issues\n",
      "https://status.python.org/\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all(\"a\"):  # look for tag 'a'\n",
    "    if link[\"href\"][0] == \"h\":  # then look for 'https' inside 'href'\n",
    "        # or: link['href'].startswith('https')\n",
    "        # or: 'https' in link['href']\n",
    "        print(link[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Print all department names that are listed at the web page: http://deanza.edu/buscs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department Websites :\n",
      "- Accounting\n",
      "- Automotive Technology\n",
      "- Business\n",
      "- Computer Information Systems\n",
      "- Design and Manufacturing Technologies (DMT) (includes Computer Aided Design and Digital Imaging; Manufacturing and CNC Technology)\n",
      "                                    \n",
      "- Real Estate\n"
     ]
    }
   ],
   "source": [
    "## MY VERSION\n",
    "\n",
    "# Print all department names that are listed at the web page: http://deanza.edu/buscs/\n",
    "# departments are a bulleted list following the header, \"Department Websites\"\n",
    "\n",
    "# Get the page and create a BeautifulSoup object with the page's content\n",
    "page_DA = requests.get(\"http://deanza.edu/buscs/\")\n",
    "soup_DA = BeautifulSoup(page_DA.content, \"lxml\")\n",
    "\n",
    "# Find the header with the text \"Department Websites\"\n",
    "dept_header = soup_DA.find(\"h3\", string=\"Department Websites\")\n",
    "\n",
    "# print the header\n",
    "print(dept_header.text, \":\")\n",
    "\n",
    "# Find the unordered list that follows the header\n",
    "dept_list = dept_header.find_next_sibling(\"ul\")\n",
    "\n",
    "# Find all the list items in the unordered list\n",
    "dept_list_items = dept_list.find_all(\"li\")\n",
    "\n",
    "# Print the text of each list item\n",
    "for item in dept_list_items:\n",
    "    print(\"-\", item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounting\n",
      "Automotive Technology\n",
      "Business\n",
      "Computer Information Systems\n",
      "Design and Manufacturing Technologies (DMT)\n",
      "Real Estate\n"
     ]
    }
   ],
   "source": [
    "## IN CLASS VERSION\n",
    "# Print all department names that are listed at the web page: http://deanza.edu/buscs/\n",
    "# departments are a bulleted list following the header, \"Department Websites\"\n",
    "\n",
    "# get the page and create a beautiful soup object\n",
    "page = requests.get(\"http://deanza.edu/buscs/\")\n",
    "soup = soup_DA = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "div = soup.find(\"div\", class_=\"col-xs-12 col-md-8 l-content\")\n",
    "for element in div.find_all(\"li\"):\n",
    "    for link in element.find_all(\"a\"):\n",
    "        print(link.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class version #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accounting\n",
      "Automotive Technology\n",
      "Business\n",
      "Computer Information Systems\n",
      "Design and Manufacturing Technologies (DMT)\n",
      "Real Estate\n"
     ]
    }
   ],
   "source": [
    "# Print all department names that are listed at the web page: http://deanza.edu/buscs/\n",
    "# departments are a bulleted list following the header, \"Department Websites\"\n",
    "\n",
    "# get the page and create a beautiful soup object\n",
    "page = requests.get(\"http://deanza.edu/buscs/\")\n",
    "soup = soup_DA = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "# find the header with the text \"Department Websites\" and print it\n",
    "for link in soup.select(\"div .col-xs-12.col-md-8.l-content li a\"):\n",
    "    print(link.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>8. Use the API of the ISS (International Space Station) to see where the ISS is currently located.\n",
    "    <li>a. google:  iss api</li>\n",
    "    <li>b. go to the link (already coded below) and fetch the data</li>\n",
    "    <li>c. print the data. What data type is it?</li>\n",
    "    <li>d. print in nice format: time, latitude, longitude    (for time, use time.ctime(t)  to convert to a character time string)</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'dict'> \n",
      "\n",
      "Current time: Tue May 16 12:29:35 2023\n",
      "Latitude: 44.5554\n",
      "Longitude: 86.1458\n"
     ]
    }
   ],
   "source": [
    "# Provided code\n",
    "page = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "data = page.json()\n",
    "\n",
    "# Print the data\n",
    "print(\"Data type:\", type(data), \"\\n\")\n",
    "\n",
    "# Extract information from the data\n",
    "timestamp = data[\"timestamp\"]\n",
    "latitude = data[\"iss_position\"][\"latitude\"]\n",
    "longitude = data[\"iss_position\"][\"longitude\"]\n",
    "\n",
    "# Print the information in a nice format\n",
    "print(\"Current time:\", time.ctime(timestamp))\n",
    "print(\"Latitude:\", latitude)\n",
    "print(\"Longitude:\", longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'iss_position': {'latitude': '46.4728', 'longitude': '81.1684'}, 'timestamp': 1684265311, 'message': 'success'} \n",
      "\n",
      "Data type: <class 'dict'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## IN CLASS VERSION\n",
    "\n",
    "page = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "data = page.json()\n",
    "print(\"Data:\", data, \"\\n\")\n",
    "print(\"Data type:\", type(data), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of going further is to install and use the geopy module: https://pypi.python.org/pypi/geopy\n",
    "which will return a map location for a particular latitude,longitude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
